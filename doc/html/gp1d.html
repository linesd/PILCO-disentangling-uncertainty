
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>gp1d.m</title><meta name="generator" content="MATLAB 8.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-06-07"><meta name="DC.source" content="gp1d.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>gp1d.m</h1><!--introduction--><p><b>Summary:</b> Compute joint predictions (and derivatives) for the FITC sparse approximation to multiple GPs with uncertain inputs. Predictive variances contain uncertainty about the function, but no noise. If gpmodel.nigp exists, individual noise contributions are added.</p><pre class="language-matlab"><span class="keyword">function</span> [M, S, V, dMdm, dSdm, dVdm, dMds, dSds, dVds] = gp1d(gpmodel, m, s)
</pre><p><b>Input arguments:</b></p><pre class="language-matlab">gpmodel    <span class="string">GP</span> <span class="string">model</span> <span class="string">struct</span>
  hyp      <span class="string">log-hyper-parameters</span>                                  <span class="string">[D+2 x  E ]</span>
  inputs   <span class="string">training</span> <span class="string">inputs</span>                                       <span class="string">[ n  x  D ]</span>
  targets  <span class="string">training</span> <span class="string">targets</span>                                      <span class="string">[ n  x  E ]</span>
  nigp     (optional) individual <span class="string">noise</span> <span class="string">variance</span> <span class="string">terms</span>            <span class="string">[ n  x  E ]</span>
m          <span class="string">mean</span> <span class="string">of</span> <span class="string">the</span> <span class="string">test</span> <span class="string">distribution</span>                         <span class="string">[ D  x  1 ]</span>
s          <span class="string">covariance</span> <span class="string">matrix</span> <span class="string">of</span> <span class="string">the</span> <span class="string">test</span> <span class="string">distribution</span>            <span class="string">[ D  x  D ]</span>
</pre><p><b>Output arguments:</b></p><pre class="language-matlab">M          <span class="string">mean</span> <span class="string">of</span> <span class="string">pred.</span> <span class="string">distribution</span>                            <span class="string">[ E  x  1 ]</span>
S          <span class="string">covariance</span> <span class="string">of</span> <span class="string">the</span> <span class="string">pred.</span> <span class="string">distribution</span>                  <span class="string">[ E  x  E ]</span>
V          <span class="string">inv(s)</span> <span class="string">times</span> <span class="string">covariance</span> <span class="string">between</span> <span class="string">input</span> <span class="string">and</span> <span class="string">output</span>      <span class="string">[ D  x  E ]</span>
dMdm       <span class="string">output</span> <span class="string">mean</span> <span class="string">by</span> <span class="string">input</span> <span class="string">mean</span>                             <span class="string">[ E  x  D ]</span>
dSdm       <span class="string">output</span> <span class="string">covariance</span> <span class="string">by</span> <span class="string">input</span> <span class="string">mean</span>                       <span class="string">[E*E x  D ]</span>
dVdm       <span class="string">inv(s)*input-output</span> <span class="string">covariance</span> <span class="string">by</span> <span class="string">input</span> <span class="string">mean</span>          <span class="string">[D*E x  D ]</span>
dMds       <span class="string">ouput</span> <span class="string">mean</span> <span class="string">by</span> <span class="string">input</span> <span class="string">covariance</span>                        <span class="string">[ E  x D*D]</span>
dSds       <span class="string">output</span> <span class="string">covariance</span> <span class="string">by</span> <span class="string">input</span> <span class="string">covariance</span>                 <span class="string">[E*E x D*D]</span>
dVds       <span class="string">inv(s)*input-output</span> <span class="string">covariance</span> <span class="string">by</span> <span class="string">input</span> <span class="string">covariance</span>    <span class="string">[D*E x D*D]</span>
</pre><p>Copyright (C) 2008-2013 by Marc Deisenroth, Andrew McHutchon, Joe Hall, and Carl Edward Rasmussen.</p><p>Last modified: 2013-05-24</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">High-Level Steps</a></li><li><a href="#2">Code</a></li></ul></div><h2>High-Level Steps<a name="1"></a></h2><div><ol><li>If necessary, compute kernel matrix and cache it</li><li>Compute predicted mean and inv(s) times input-output covariance</li><li>Compute predictive covariance matrix, non-central moments</li><li>Centralize moments</li><li>Vectorize derivatives</li></ol></div><pre class="codeinput"><span class="keyword">function</span> [M, S, V, dMdm, dSdm, dVdm, dMds, dSds, dVds] = gp1d(gpmodel, m, s)
</pre><h2>Code<a name="2"></a></h2><pre class="codeinput"><span class="comment">% If no derivatives are required, call gp1</span>
<span class="keyword">if</span> nargout &lt; 4; [M S V] = gp1(gpmodel, m, s); <span class="keyword">return</span>; <span class="keyword">end</span>
<span class="comment">% If there are no inducing inputs, back off to gp0d (no sparse GP required)</span>
<span class="keyword">if</span> numel(gpmodel.induce) == 0
  [M S V dMdm dSdm dVdm dMds dSds dVds] = gp0d(gpmodel, m, s); <span class="keyword">return</span>;
<span class="keyword">end</span>

<span class="comment">% 1) If necessary: re-compute cached variables</span>
<span class="keyword">persistent</span> iK iK2 beta oldX;
ridge = 1e-6;                        <span class="comment">% jitter to make matrix better conditioned</span>
[n, D] = size(gpmodel.inputs);    <span class="comment">% number of examples and dimension of inputs</span>
E = size(gpmodel.targets,2);         <span class="comment">% number of examples and number of outputs</span>
X = gpmodel.hyp; input = gpmodel.inputs; targets = gpmodel.targets;

[np pD pE] = size(gpmodel.induce);     <span class="comment">% number of pseudo inputs per dimension</span>
pinput = gpmodel.induce;                                   <span class="comment">% all pseudo inputs</span>

<span class="keyword">if</span> numel(X) ~= numel(oldX) || isempty(iK) || isempty(iK2) || <span class="keyword">...</span><span class="comment"> % if necessary</span>
    sum(any(X ~= oldX)) || numel(iK2) ~=E*np^2 || numel(iK) ~= n*np*E
  oldX = X;                                        <span class="comment">% compute K, inv(K), inv(K2)</span>
  iK = zeros(np,n,E); iK2 = zeros(np,np,E); beta = zeros(np,E);

  <span class="keyword">for</span> i = 1:E
    pinp = bsxfun(@rdivide,pinput(:,:,min(i,pE)),exp(X(1:D,i)'));
    inp = bsxfun(@rdivide,input,exp(X(1:D,i)'));
    Kmm = exp(2*X(D+1,i)-maha(pinp,pinp)/2) + ridge*eye(np);  <span class="comment">% add small ridge</span>
    Kmn = exp(2*X(D+1,i)-maha(pinp,inp)/2);
    L = chol(Kmm)';
    V = L\Kmn;                                             <span class="comment">% inv(sqrt(Kmm))*Kmn</span>
    <span class="keyword">if</span> isfield(gpmodel,<span class="string">'nigp'</span>)
      G = exp(2*X(D+1,i))-sum(V.^2)+gpmodel.nigp(:,i)';
    <span class="keyword">else</span>
      G = exp(2*X(D+1,i))-sum(V.^2);
    <span class="keyword">end</span>
    G = sqrt(1+G/exp(2*X(D+2,i)));
    V = bsxfun(@rdivide,V,G);
    Am = chol(exp(2*X(D+2,i))*eye(np) + V*V')';
    At = L*Am;                           <span class="comment">% chol(sig*B) [Snelson's thesis, p. 40]</span>
    iAt = At\eye(np);
    <span class="comment">% The following is not an inverse matrix, but we'll treat it as such: multiply</span>
    <span class="comment">% the targets from right and the cross-covariances left to get predictive mean.</span>
    iK(:,:,i) = ((Am\(bsxfun(@rdivide,V,G)))'*iAt)';
    beta(:,i) = iK(:,:,i)*targets(:,i);
    iB = iAt'*iAt.*exp(2*X(D+2,i));          <span class="comment">% inv(B), [Snelson's thesis, p. 40]</span>
    iK2(:,:,i) = Kmm\eye(np) - iB; <span class="comment">% covariance matrix for predictive variances</span>
  <span class="keyword">end</span>
<span class="keyword">end</span>

k = zeros(np,E); M = zeros(E,1); V = zeros(D,E); S = zeros(E);       <span class="comment">% allocate</span>
dMds = zeros(E,D,D); dSdm = zeros(E,E,D); r = zeros(1,D);
dSds = zeros(E,E,D,D); dVds = zeros(D,E,D,D); T = zeros(D);
inp = zeros(np,D,E);

<span class="comment">% 2) Compute predicted mean and inv(s) times input-output covariance</span>
<span class="keyword">for</span> i = 1:E
  inp(:,:,i) = bsxfun(@minus,pinput(:,:,min(i,pE)),m');   <span class="comment">% centralize p-inputs</span>

  L = diag(exp(-X(1:D,i)));
  in = inp(:,:,i)*L;
  B = L*s*L+eye(D);  LiBL = L/B*L; <span class="comment">% iR = LiBL;</span>

  t = in/B;
  l = exp(-sum(in.*t,2)/2); lb = l.*beta(:,i);
  tL = t*L;
  tlb = bsxfun(@times,tL,lb);
  c = exp(2*X(D+1,i))/sqrt(det(B));
  M(i) = c*sum(lb);
  V(:,i) = tL'*lb*c;                   <span class="comment">% inv(s) times input-output covariance</span>
  dMds(i,:,:) = c*tL'*tlb/2 - LiBL*M(i)/2;
  <span class="keyword">for</span> d = 1:D
    dVds(d,i,:,:) = c*bsxfun(@times,tL,tL(:,d))'*tlb/2 - LiBL*V(d,i)/2 <span class="keyword">...</span>
      - (V(:,i)*LiBL(d,:) + LiBL(:,d)*V(:,i)')/2;
  <span class="keyword">end</span>
  k(:,i) = 2*X(D+1,i)-sum(in.*in,2)/2;
<span class="keyword">end</span>
dMdm = V'; dVdm = 2*permute(dMds,[2 1 3]);                  <span class="comment">% derivatives wrt m</span>


iell2 = exp(-2*gpmodel.hyp(1:D,:));
inpiell2 = bsxfun(@times,inp,permute(iell2,[3,1,2])); <span class="comment">% N-by-D-by-E</span>

<span class="comment">% 3) Compute predictive covariance matrix, non-central moments</span>
<span class="keyword">for</span> i=1:E
  ii = inpiell2(:,:,i);

  <span class="keyword">for</span> j=1:i
    R = s*diag(iell2(:,i)+iell2(:,j))+eye(D); t = 1/sqrt(det(R));
    ij = inpiell2(:,:,j);
    L = exp(bsxfun(@plus,k(:,i),k(:,j)')+maha(ii,-ij,R\s/2));

    <span class="keyword">if</span> i==j
      iKL = iK2(:,:,i).*L; s1iKL = sum(iKL,1); s2iKL = sum(iKL,2);
      S(i,j) = t*(beta(:,i)'*L*beta(:,i) - sum(s1iKL));
      zi = ii/R;
      bibLi = L'*beta(:,i).*beta(:,i); cbLi = L'*bsxfun(@times, beta(:,i), zi);
      r = (bibLi'*zi*2 - (s2iKL' + s1iKL)*zi)*t;
      <span class="keyword">for</span> d = 1:D
        T(d,1:d) = 2*(zi(:,1:d)'*(zi(:,d).*bibLi) + <span class="keyword">...</span>
          cbLi(:,1:d)'*(zi(:,d).*beta(:,i)) - zi(:,1:d)'*(zi(:,d).*s2iKL) <span class="keyword">...</span>
          - zi(:,1:d)'*(iKL*zi(:,d)));
        T(1:d,d) = T(d,1:d)';
      <span class="keyword">end</span>
    <span class="keyword">else</span>
      zi = ii/R; zj = ij/R;
      S(i,j) = beta(:,i)'*L*beta(:,j)*t; S(j,i) = S(i,j);

      bibLj = L*beta(:,j).*beta(:,i); bjbLi = L'*beta(:,i).*beta(:,j);
      cbLi = L'*bsxfun(@times, beta(:,i), zi);
      cbLj = L*bsxfun(@times, beta(:,j), zj);

      r = (bibLj'*zi+bjbLi'*zj)*t;
      <span class="keyword">for</span> d = 1:D
        T(d,1:d) = zi(:,1:d)'*(zi(:,d).*bibLj) + <span class="keyword">...</span>
          cbLi(:,1:d)'*(zj(:,d).*beta(:,j)) + zj(:,1:d)'*(zj(:,d).*bjbLi) + <span class="keyword">...</span>
          cbLj(:,1:d)'*(zi(:,d).*beta(:,i));
        T(1:d,d) = T(d,1:d)';
      <span class="keyword">end</span>
    <span class="keyword">end</span>

    dSdm(i,j,:) = r - M(i)*dMdm(j,:)-M(j)*dMdm(i,:); dSdm(j,i,:) = dSdm(i,j,:);
    T = (t*T-S(i,j)*diag(iell2(:,i)+iell2(:,j))/R)/2;
    T = T - squeeze(M(i)*dMds(j,:,:) + M(j)*dMds(i,:,:));
    dSds(i,j,:,:) = T; dSds(j,i,:,:) = T;
  <span class="keyword">end</span>

  S(i,i) = S(i,i) + exp(2*X(D+1,i));
<span class="keyword">end</span>

<span class="comment">% 4) Centralize moments</span>
S = S - M*M';

<span class="comment">% 5) Vectorize derivatives</span>
dMds = reshape(dMds,[E D*D]);
dSds = reshape(dSds,[E*E D*D]); dSdm = reshape(dSdm,[E*E D]);
dVds = reshape(dVds,[D*E D*D]); dVdm = reshape(dVdm,[D*E D]);
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2012b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% gp1d.m
% *Summary:* Compute joint predictions (and derivatives) for the FITC sparse
% approximation to multiple GPs with uncertain inputs.
% Predictive variances contain uncertainty about the function, but no noise.
% If gpmodel.nigp exists, individual noise contributions are added.
%
%   function [M, S, V, dMdm, dSdm, dVdm, dMds, dSds, dVds] = gp1d(gpmodel, m, s)
%
% *Input arguments:*
%
%   gpmodel    GP model struct
%     hyp      log-hyper-parameters                                  [D+2 x  E ]
%     inputs   training inputs                                       [ n  x  D ]
%     targets  training targets                                      [ n  x  E ]
%     nigp     (optional) individual noise variance terms            [ n  x  E ]
%   m          mean of the test distribution                         [ D  x  1 ]
%   s          covariance matrix of the test distribution            [ D  x  D ]
%
% *Output arguments:*
%
%   M          mean of pred. distribution                            [ E  x  1 ]
%   S          covariance of the pred. distribution                  [ E  x  E ]
%   V          inv(s) times covariance between input and output      [ D  x  E ]
%   dMdm       output mean by input mean                             [ E  x  D ]
%   dSdm       output covariance by input mean                       [E*E x  D ]
%   dVdm       inv(s)*input-output covariance by input mean          [D*E x  D ]
%   dMds       ouput mean by input covariance                        [ E  x D*D]
%   dSds       output covariance by input covariance                 [E*E x D*D]
%   dVds       inv(s)*input-output covariance by input covariance    [D*E x D*D]
%
%
% Copyright (C) 2008-2013 by
% Marc Deisenroth, Andrew McHutchon, Joe Hall, and Carl Edward Rasmussen.
%
% Last modified: 2013-05-24
%
%% High-Level Steps
% # If necessary, compute kernel matrix and cache it
% # Compute predicted mean and inv(s) times input-output covariance
% # Compute predictive covariance matrix, non-central moments
% # Centralize moments
% # Vectorize derivatives

function [M, S, V, dMdm, dSdm, dVdm, dMds, dSds, dVds] = gp1d(gpmodel, m, s)
%% Code

% If no derivatives are required, call gp1
if nargout < 4; [M S V] = gp1(gpmodel, m, s); return; end
% If there are no inducing inputs, back off to gp0d (no sparse GP required)
if numel(gpmodel.induce) == 0
  [M S V dMdm dSdm dVdm dMds dSds dVds] = gp0d(gpmodel, m, s); return;
end

% 1) If necessary: re-compute cached variables
persistent iK iK2 beta oldX;
ridge = 1e-6;                        % jitter to make matrix better conditioned
[n, D] = size(gpmodel.inputs);    % number of examples and dimension of inputs
E = size(gpmodel.targets,2);         % number of examples and number of outputs
X = gpmodel.hyp; input = gpmodel.inputs; targets = gpmodel.targets;

[np pD pE] = size(gpmodel.induce);     % number of pseudo inputs per dimension
pinput = gpmodel.induce;                                   % all pseudo inputs

if numel(X) ~= numel(oldX) || isempty(iK) || isempty(iK2) || ... % if necessary
    sum(any(X ~= oldX)) || numel(iK2) ~=E*np^2 || numel(iK) ~= n*np*E
  oldX = X;                                        % compute K, inv(K), inv(K2)
  iK = zeros(np,n,E); iK2 = zeros(np,np,E); beta = zeros(np,E);
  
  for i = 1:E
    pinp = bsxfun(@rdivide,pinput(:,:,min(i,pE)),exp(X(1:D,i)'));
    inp = bsxfun(@rdivide,input,exp(X(1:D,i)'));
    Kmm = exp(2*X(D+1,i)-maha(pinp,pinp)/2) + ridge*eye(np);  % add small ridge
    Kmn = exp(2*X(D+1,i)-maha(pinp,inp)/2);
    L = chol(Kmm)';
    V = L\Kmn;                                             % inv(sqrt(Kmm))*Kmn
    if isfield(gpmodel,'nigp')
      G = exp(2*X(D+1,i))-sum(V.^2)+gpmodel.nigp(:,i)';
    else
      G = exp(2*X(D+1,i))-sum(V.^2);
    end
    G = sqrt(1+G/exp(2*X(D+2,i)));
    V = bsxfun(@rdivide,V,G);
    Am = chol(exp(2*X(D+2,i))*eye(np) + V*V')';
    At = L*Am;                           % chol(sig*B) [Snelson's thesis, p. 40]
    iAt = At\eye(np);
    % The following is not an inverse matrix, but we'll treat it as such: multiply
    % the targets from right and the cross-covariances left to get predictive mean.
    iK(:,:,i) = ((Am\(bsxfun(@rdivide,V,G)))'*iAt)';
    beta(:,i) = iK(:,:,i)*targets(:,i);
    iB = iAt'*iAt.*exp(2*X(D+2,i));          % inv(B), [Snelson's thesis, p. 40]
    iK2(:,:,i) = Kmm\eye(np) - iB; % covariance matrix for predictive variances
  end
end

k = zeros(np,E); M = zeros(E,1); V = zeros(D,E); S = zeros(E);       % allocate
dMds = zeros(E,D,D); dSdm = zeros(E,E,D); r = zeros(1,D);
dSds = zeros(E,E,D,D); dVds = zeros(D,E,D,D); T = zeros(D);
inp = zeros(np,D,E);

% 2) Compute predicted mean and inv(s) times input-output covariance
for i = 1:E
  inp(:,:,i) = bsxfun(@minus,pinput(:,:,min(i,pE)),m');   % centralize p-inputs
  
  L = diag(exp(-X(1:D,i)));
  in = inp(:,:,i)*L;
  B = L*s*L+eye(D);  LiBL = L/B*L; % iR = LiBL;
  
  t = in/B;
  l = exp(-sum(in.*t,2)/2); lb = l.*beta(:,i);
  tL = t*L;
  tlb = bsxfun(@times,tL,lb);
  c = exp(2*X(D+1,i))/sqrt(det(B));
  M(i) = c*sum(lb);
  V(:,i) = tL'*lb*c;                   % inv(s) times input-output covariance
  dMds(i,:,:) = c*tL'*tlb/2 - LiBL*M(i)/2;
  for d = 1:D
    dVds(d,i,:,:) = c*bsxfun(@times,tL,tL(:,d))'*tlb/2 - LiBL*V(d,i)/2 ...
      - (V(:,i)*LiBL(d,:) + LiBL(:,d)*V(:,i)')/2;
  end
  k(:,i) = 2*X(D+1,i)-sum(in.*in,2)/2;
end
dMdm = V'; dVdm = 2*permute(dMds,[2 1 3]);                  % derivatives wrt m


iell2 = exp(-2*gpmodel.hyp(1:D,:));
inpiell2 = bsxfun(@times,inp,permute(iell2,[3,1,2])); % N-by-D-by-E

% 3) Compute predictive covariance matrix, non-central moments
for i=1:E
  ii = inpiell2(:,:,i);
  
  for j=1:i
    R = s*diag(iell2(:,i)+iell2(:,j))+eye(D); t = 1/sqrt(det(R));
    ij = inpiell2(:,:,j);
    L = exp(bsxfun(@plus,k(:,i),k(:,j)')+maha(ii,-ij,R\s/2));
    
    if i==j
      iKL = iK2(:,:,i).*L; s1iKL = sum(iKL,1); s2iKL = sum(iKL,2);
      S(i,j) = t*(beta(:,i)'*L*beta(:,i) - sum(s1iKL));
      zi = ii/R;
      bibLi = L'*beta(:,i).*beta(:,i); cbLi = L'*bsxfun(@times, beta(:,i), zi);
      r = (bibLi'*zi*2 - (s2iKL' + s1iKL)*zi)*t;
      for d = 1:D
        T(d,1:d) = 2*(zi(:,1:d)'*(zi(:,d).*bibLi) + ...
          cbLi(:,1:d)'*(zi(:,d).*beta(:,i)) - zi(:,1:d)'*(zi(:,d).*s2iKL) ...
          - zi(:,1:d)'*(iKL*zi(:,d)));
        T(1:d,d) = T(d,1:d)';
      end
    else
      zi = ii/R; zj = ij/R;
      S(i,j) = beta(:,i)'*L*beta(:,j)*t; S(j,i) = S(i,j);
      
      bibLj = L*beta(:,j).*beta(:,i); bjbLi = L'*beta(:,i).*beta(:,j);
      cbLi = L'*bsxfun(@times, beta(:,i), zi);
      cbLj = L*bsxfun(@times, beta(:,j), zj);
      
      r = (bibLj'*zi+bjbLi'*zj)*t;
      for d = 1:D
        T(d,1:d) = zi(:,1:d)'*(zi(:,d).*bibLj) + ...
          cbLi(:,1:d)'*(zj(:,d).*beta(:,j)) + zj(:,1:d)'*(zj(:,d).*bjbLi) + ...
          cbLj(:,1:d)'*(zi(:,d).*beta(:,i));
        T(1:d,d) = T(d,1:d)';
      end
    end
    
    dSdm(i,j,:) = r - M(i)*dMdm(j,:)-M(j)*dMdm(i,:); dSdm(j,i,:) = dSdm(i,j,:);
    T = (t*T-S(i,j)*diag(iell2(:,i)+iell2(:,j))/R)/2;
    T = T - squeeze(M(i)*dMds(j,:,:) + M(j)*dMds(i,:,:));
    dSds(i,j,:,:) = T; dSds(j,i,:,:) = T;
  end
  
  S(i,i) = S(i,i) + exp(2*X(D+1,i));
end

% 4) Centralize moments
S = S - M*M';

% 5) Vectorize derivatives
dMds = reshape(dMds,[E D*D]);
dSds = reshape(dSds,[E*E D*D]); dSdm = reshape(dSdm,[E*E D]);
dVds = reshape(dVds,[D*E D*D]); dVdm = reshape(dVdm,[D*E D]);
##### SOURCE END #####
--></body></html>