
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>gp1.m</title><meta name="generator" content="MATLAB 8.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-06-07"><meta name="DC.source" content="gp1.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>gp1.m</h1><!--introduction--><p><b>Summary:</b> Compute joint predictions for the FITC sparse approximation to multiple GPs with uncertain inputs. Predictive variances contain uncertainty about the function, but no noise. If gpmodel.nigp exists, individual noise contributions are added.</p><pre class="language-matlab"><span class="keyword">function</span> [M, S, V] = gp1d(gpmodel, m, s)
</pre><p><b>Input arguments:</b></p><pre class="language-matlab">gpmodel    <span class="string">GP</span> <span class="string">model</span> <span class="string">struct</span>
  hyp      <span class="string">log-hyper-parameters</span>                                  <span class="string">[D+2 x  E ]</span>
  inputs   <span class="string">training</span> <span class="string">inputs</span>                                       <span class="string">[ n  x  D ]</span>
  targets  <span class="string">training</span> <span class="string">targets</span>                                      <span class="string">[ n  x  E ]</span>
  nigp     (optional) individual <span class="string">noise</span> <span class="string">variance</span> <span class="string">terms</span>            <span class="string">[ n  x  E ]</span>
m          <span class="string">mean</span> <span class="string">of</span> <span class="string">the</span> <span class="string">test</span> <span class="string">distribution</span>                         <span class="string">[ D  x  1 ]</span>
s          <span class="string">covariance</span> <span class="string">matrix</span> <span class="string">of</span> <span class="string">the</span> <span class="string">test</span> <span class="string">distribution</span>            <span class="string">[ D  x  D ]</span>
</pre><p><b>Output arguments:</b></p><pre class="language-matlab">M          <span class="string">mean</span> <span class="string">of</span> <span class="string">pred.</span> <span class="string">distribution</span>                            <span class="string">[ E  x  1 ]</span>
S          <span class="string">covariance</span> <span class="string">of</span> <span class="string">the</span> <span class="string">pred.</span> <span class="string">distribution</span>                  <span class="string">[ E  x  E ]</span>
V          <span class="string">inv(s)</span> <span class="string">times</span> <span class="string">covariance</span> <span class="string">between</span> <span class="string">input</span> <span class="string">and</span> <span class="string">output</span>      <span class="string">[ D  x  E ]</span>
</pre><p>Copyright (C) 2008-2013 by Marc Deisenroth, Andrew McHutchon, Joe Hall, and Carl Edward Rasmussen.</p><p>Last modified: 2013-03-05</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">High-Level Steps</a></li><li><a href="#2">Code</a></li></ul></div><h2>High-Level Steps<a name="1"></a></h2><div><ol><li>If necessary, compute kernel matrix and cache it</li><li>Compute predicted mean and inv(s) times input-output covariance</li><li>Compute predictive covariance matrix, non-central moments</li><li>Centralize moments</li></ol></div><pre class="codeinput"><span class="keyword">function</span> [M, S, V] = gp1(gpmodel, m, s)
</pre><h2>Code<a name="2"></a></h2><pre class="codeinput"><span class="keyword">if</span> ~isfield(gpmodel,<span class="string">'induce'</span>) || numel(gpmodel.induce)==0,
    [M, S, V] = gp0(gpmodel, m, s); <span class="keyword">return</span>; <span class="keyword">end</span>

<span class="keyword">persistent</span> iK iK2 beta oldX;
ridge = 1e-6;                        <span class="comment">% jitter to make matrix better conditioned</span>
[n, D] = size(gpmodel.inputs);    <span class="comment">% number of examples and dimension of inputs</span>
E = size(gpmodel.targets,2);         <span class="comment">% number of examples and number of outputs</span>
X = gpmodel.hyp; input = gpmodel.inputs; targets = gpmodel.targets;

[np pD pE] = size(gpmodel.induce);     <span class="comment">% number of pseudo inputs per dimension</span>
pinput = gpmodel.induce;                                   <span class="comment">% all pseudo inputs</span>

<span class="comment">% 1) If necessary: re-compute cached variables</span>
<span class="keyword">if</span> numel(X) ~= numel(oldX) || isempty(iK) || isempty(iK2) || <span class="keyword">...</span><span class="comment"> % if necessary</span>
              sum(any(X ~= oldX)) || numel(iK2) ~=E*np^2 || numel(iK) ~= n*np*E
  oldX = X;                                        <span class="comment">% compute K, inv(K), inv(K2)</span>
  iK = zeros(np,n,E); iK2 = zeros(np,np,E); beta = zeros(np,E);

  <span class="keyword">for</span> i=1:E
    pinp = bsxfun(@rdivide,pinput(:,:,min(i,pE)),exp(X(1:D,i)'));
    inp = bsxfun(@rdivide,input,exp(X(1:D,i)'));
    Kmm = exp(2*X(D+1,i)-maha(pinp,pinp)/2) + ridge*eye(np);  <span class="comment">% add small ridge</span>
    Kmn = exp(2*X(D+1,i)-maha(pinp,inp)/2);
    L = chol(Kmm)';
    V = L\Kmn;                                             <span class="comment">% inv(sqrt(Kmm))*Kmn</span>
    <span class="keyword">if</span> isfield(gpmodel,<span class="string">'nigp'</span>)
      G = exp(2*X(D+1,i))-sum(V.^2)+gpmodel.nigp(:,i)';
    <span class="keyword">else</span>
      G = exp(2*X(D+1,i))-sum(V.^2);
    <span class="keyword">end</span>
    G = sqrt(1+G/exp(2*X(D+2,i)));
    V = bsxfun(@rdivide,V,G);
    Am = chol(exp(2*X(D+2,i))*eye(np) + V*V')';
    At = L*Am;                                    <span class="comment">% chol(sig*B) [thesis, p. 40]</span>
    iAt = At\eye(np);
<span class="comment">% The following is not an inverse matrix, but we'll treat it as such: multiply</span>
<span class="comment">% the targets from right and the cross-covariances left to get predictive mean.</span>
    iK(:,:,i) = ((Am\(bsxfun(@rdivide,V,G)))'*iAt)';
    beta(:,i) = iK(:,:,i)*targets(:,i);
    iB = iAt'*iAt.*exp(2*X(D+2,i));              <span class="comment">% inv(B), [Ed's thesis, p. 40]</span>
    iK2(:,:,i) = Kmm\eye(np) - iB; <span class="comment">% covariance matrix for predictive variances</span>
  <span class="keyword">end</span>
<span class="keyword">end</span>

k = zeros(np,E); M = zeros(E,1); V = zeros(D,E); S = zeros(E);       <span class="comment">% allocate</span>
inp = zeros(np,D,E);

<span class="comment">% 2) Compute predicted mean and inv(s) times input-output covariance</span>
<span class="keyword">for</span> i=1:E
  inp(:,:,i) = bsxfun(@minus,pinput(:,:,min(i,pE)),m');

  L = diag(exp(-X(1:D,i)));
  in = inp(:,:,i)*L;
  B = L*s*L+eye(D);

  t = in/B;
  l = exp(-sum(in.*t,2)/2); lb = l.*beta(:,i);
  tL = t*L;
  c = exp(2*X(D+1,i))/sqrt(det(B));

  M(i) = sum(lb)*c;                                            <span class="comment">% predicted mean</span>
  V(:,i) = tL'*lb*c;                     <span class="comment">% inv(s) times input-output covariance</span>
  k(:,i) = 2*X(D+1,i)-sum(in.*in,2)/2;
<span class="keyword">end</span>

<span class="comment">% 3) Compute predictive covariance matrix, non-central moments</span>
<span class="keyword">for</span> i=1:E
  ii = bsxfun(@rdivide,inp(:,:,i),exp(2*X(1:D,i)'));

  <span class="keyword">for</span> j=1:i
    R = s*diag(exp(-2*X(1:D,i))+exp(-2*X(1:D,j)))+eye(D); t = 1./sqrt(det(R));
    ij = bsxfun(@rdivide,inp(:,:,j),exp(2*X(1:D,j)'));
    L = exp(bsxfun(@plus,k(:,i),k(:,j)')+maha(ii,-ij,R\s/2));
    <span class="keyword">if</span> i==j
      S(i,i) = t*(beta(:,i)'*L*beta(:,i) - sum(sum(iK2(:,:,i).*L)));
    <span class="keyword">else</span>
      S(i,j) = beta(:,i)'*L*beta(:,j)*t; S(j,i) = S(i,j);
    <span class="keyword">end</span>
  <span class="keyword">end</span>

  S(i,i) = S(i,i) + exp(2*X(D+1,i));
<span class="keyword">end</span>

<span class="comment">% 4) Centralize moments</span>
S = S - M*M';
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2012b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% gp1.m
% *Summary:* Compute joint predictions for the FITC sparse approximation to 
% multiple GPs with uncertain inputs. 
% Predictive variances contain uncertainty about the function, but no noise. 
% If gpmodel.nigp exists, individual noise contributions are added.
%
%   function [M, S, V] = gp1d(gpmodel, m, s)
% 
% *Input arguments:*
%
%   gpmodel    GP model struct
%     hyp      log-hyper-parameters                                  [D+2 x  E ]
%     inputs   training inputs                                       [ n  x  D ]
%     targets  training targets                                      [ n  x  E ]
%     nigp     (optional) individual noise variance terms            [ n  x  E ]
%   m          mean of the test distribution                         [ D  x  1 ]
%   s          covariance matrix of the test distribution            [ D  x  D ]
%
% *Output arguments:*
%
%   M          mean of pred. distribution                            [ E  x  1 ]
%   S          covariance of the pred. distribution                  [ E  x  E ]
%   V          inv(s) times covariance between input and output      [ D  x  E ]
%
%
% Copyright (C) 2008-2013 by
% Marc Deisenroth, Andrew McHutchon, Joe Hall, and Carl Edward Rasmussen.
%
% Last modified: 2013-03-05
%
%% High-Level Steps
% # If necessary, compute kernel matrix and cache it
% # Compute predicted mean and inv(s) times input-output covariance
% # Compute predictive covariance matrix, non-central moments
% # Centralize moments

function [M, S, V] = gp1(gpmodel, m, s)
%% Code
if ~isfield(gpmodel,'induce') || numel(gpmodel.induce)==0, 
    [M, S, V] = gp0(gpmodel, m, s); return; end

persistent iK iK2 beta oldX;
ridge = 1e-6;                        % jitter to make matrix better conditioned
[n, D] = size(gpmodel.inputs);    % number of examples and dimension of inputs
E = size(gpmodel.targets,2);         % number of examples and number of outputs
X = gpmodel.hyp; input = gpmodel.inputs; targets = gpmodel.targets;

[np pD pE] = size(gpmodel.induce);     % number of pseudo inputs per dimension
pinput = gpmodel.induce;                                   % all pseudo inputs

% 1) If necessary: re-compute cached variables
if numel(X) ~= numel(oldX) || isempty(iK) || isempty(iK2) || ... % if necessary
              sum(any(X ~= oldX)) || numel(iK2) ~=E*np^2 || numel(iK) ~= n*np*E
  oldX = X;                                        % compute K, inv(K), inv(K2)
  iK = zeros(np,n,E); iK2 = zeros(np,np,E); beta = zeros(np,E);
    
  for i=1:E
    pinp = bsxfun(@rdivide,pinput(:,:,min(i,pE)),exp(X(1:D,i)'));
    inp = bsxfun(@rdivide,input,exp(X(1:D,i)'));
    Kmm = exp(2*X(D+1,i)-maha(pinp,pinp)/2) + ridge*eye(np);  % add small ridge
    Kmn = exp(2*X(D+1,i)-maha(pinp,inp)/2);
    L = chol(Kmm)';
    V = L\Kmn;                                             % inv(sqrt(Kmm))*Kmn
    if isfield(gpmodel,'nigp')
      G = exp(2*X(D+1,i))-sum(V.^2)+gpmodel.nigp(:,i)';
    else
      G = exp(2*X(D+1,i))-sum(V.^2);
    end
    G = sqrt(1+G/exp(2*X(D+2,i)));
    V = bsxfun(@rdivide,V,G);
    Am = chol(exp(2*X(D+2,i))*eye(np) + V*V')';
    At = L*Am;                                    % chol(sig*B) [thesis, p. 40]
    iAt = At\eye(np);
% The following is not an inverse matrix, but we'll treat it as such: multiply
% the targets from right and the cross-covariances left to get predictive mean.
    iK(:,:,i) = ((Am\(bsxfun(@rdivide,V,G)))'*iAt)';
    beta(:,i) = iK(:,:,i)*targets(:,i);      
    iB = iAt'*iAt.*exp(2*X(D+2,i));              % inv(B), [Ed's thesis, p. 40]
    iK2(:,:,i) = Kmm\eye(np) - iB; % covariance matrix for predictive variances       
  end
end

k = zeros(np,E); M = zeros(E,1); V = zeros(D,E); S = zeros(E);       % allocate
inp = zeros(np,D,E);

% 2) Compute predicted mean and inv(s) times input-output covariance
for i=1:E    
  inp(:,:,i) = bsxfun(@minus,pinput(:,:,min(i,pE)),m');
 
  L = diag(exp(-X(1:D,i)));
  in = inp(:,:,i)*L;
  B = L*s*L+eye(D); 
  
  t = in/B;
  l = exp(-sum(in.*t,2)/2); lb = l.*beta(:,i);
  tL = t*L;
  c = exp(2*X(D+1,i))/sqrt(det(B));
  
  M(i) = sum(lb)*c;                                            % predicted mean
  V(:,i) = tL'*lb*c;                     % inv(s) times input-output covariance
  k(:,i) = 2*X(D+1,i)-sum(in.*in,2)/2;
end

% 3) Compute predictive covariance matrix, non-central moments
for i=1:E          
  ii = bsxfun(@rdivide,inp(:,:,i),exp(2*X(1:D,i)'));
  
  for j=1:i
    R = s*diag(exp(-2*X(1:D,i))+exp(-2*X(1:D,j)))+eye(D); t = 1./sqrt(det(R));
    ij = bsxfun(@rdivide,inp(:,:,j),exp(2*X(1:D,j)'));
    L = exp(bsxfun(@plus,k(:,i),k(:,j)')+maha(ii,-ij,R\s/2));
    if i==j
      S(i,i) = t*(beta(:,i)'*L*beta(:,i) - sum(sum(iK2(:,:,i).*L)));
    else
      S(i,j) = beta(:,i)'*L*beta(:,j)*t; S(j,i) = S(i,j);
    end  
  end

  S(i,i) = S(i,i) + exp(2*X(D+1,i));
end

% 4) Centralize moments
S = S - M*M';                                              

##### SOURCE END #####
--></body></html>