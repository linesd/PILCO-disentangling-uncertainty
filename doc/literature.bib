% This file was created with JabRef 2.9b2.
% Encoding: UTF-8

@INPROCEEDINGS{Bagnell2001,
  author = {James A. Bagnell and Jeff G. Schneider},
  title = {Autonomous {Helicopter Control using Reinforcement Learning Policy
	Search Methods}},
  booktitle = {Proceedings of the International Conference on Robotics and Automation},
  year = {2001},
  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.6233},
  doi = {10.1.1.26.6233},
  owner = {marc},
  timestamp = {2013.04.16},
  url = {http://www.ri.cmu.edu/pub_files/pub3/bagnell_james_2001_2/bagnell_james_2001_2.pdf}
}

@BOOK{Deisenroth2010b,
  title = {Efficient {Reinforcement Learning using Gaussian Processes}},
  publisher = {KIT Scientific Publishing},
  year = {2010},
  editor = {U. D. Hanebeck},
  author = {Marc P. Deisenroth},
  volume = {9},
  series = {Karlsruhe Series on Intelligent Sensor-Actuator-Systems},
  month = {November},
  note = {ISBN 978-3-86644-569-7},
  abstract = {This book examines Gaussian processes (GPs) in model-based
	
	reinforcement learning (RL) and inference in nonlinear dynamic
	
	systems.
	
	
	First, we introduce PILCO, a fully Bayesian approach for efficient
	RL
	
	in continuous-valued state and action spaces when no expert knowledge
	
	is available. PILCO learns fast since it takes model uncertainties
	
	consistently into account during long-term planning and decision
	
	making. Thus, it reduces model bias, a common problem in model-based
	
	RL. Due to its generality and efficiency, PILCO is a conceptual and
	
	practical approach to jointly learning models and controllers fully
	
	automatically. Across all tasks, we report an unprecedented degree
	of
	
	automation and an unprecedented speed of learning.
	
	
	Second, we propose principled algorithms for robust filtering and
	
	smoothing in GP dynamic systems. Our methods are based on analytic
	
	moment matching and clearly advance state-of-the-art methods.},
  owner = {marc},
  timestamp = {2010.11.17},
  url = {http://www.ksp.kit.edu/shop/isbn2shopid.php?isbn=978-3-86644-569-7}
}

@INPROCEEDINGS{Deisenroth2011c,
  author = {Marc P. Deisenroth and Carl E. Rasmussen},
  title = {P{ILCO: A Model-Based and Data-Efficient Approach to Policy Search}},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  year = {2011},
  pages = {465--472},
  address = {New York, NY, USA},
  month = {June},
  publisher = {ACM},
  abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based
	policy search method. PILCO reduces model bias, one of the key problems
	of model-based reinforcement learning, in a principled way. By learning
	a probabilistic dynamics model and explicitly incorporating model
	uncertainty into long-term planning, PILCO can cope with very little
	data and facilitates learning from scratch in only a few trials.
	Policy evaluation is performed in closed form using state-of-the-art
	approximate inference. Furthermore, policy gradients are computed
	analytically for policy improvement. We report unprecedented learning
	efficiency on challenging and high-dimensional control tasks.},
  owner = {marc},
  timestamp = {2011.01.20}
}

@INPROCEEDINGS{Deisenroth2011b,
  author = {Marc P. Deisenroth and Carl E. Rasmussen and Dieter Fox},
  title = {Learning {to Control a Low-Cost Manipulator using Data-Efficient
	Reinforcement Learning}},
  booktitle = {Proceedings of the International Conference on Robotics: Science
	and Systems},
  year = {2011},
  address = {Los Angeles, CA, USA},
  month = {June},
  abstract = {Over the last years, there has been substantial progress in robust
	manipulation in unstructured environments. The long-term goal of
	our work is to get away from precise, but very expensive robotic
	systems and to develop affordable, potentially imprecise, self-adaptive
	manipulator systems that can interactively perform tasks such as
	playing with children. In this paper, we demonstrate how a low-cost
	off-the-shelf robotic system can learn closed-loop policies for a
	stacking task in only a handful of trials-from scratch. Our manipulator
	is inaccurate and provides no pose feedback. For learning a controller
	in the work space of a Kinect-style depth camera, we use a model-based
	reinforcement learning technique. Our learning method is data efficient,
	reduces model bias, and deals with several noise sources in a principled
	way during long-term planning. We present a way of incorporating
	state-space constraints into the learning process and analyze the
	learning gain by exploiting the sequential structure of the stacking
	task.},
  owner = {marc},
  timestamp = {2011.01.20},
  url = {http://www.roboticsproceedings.org/rss07/p08-pdf.html}
}

@UNPUBLISHED{Forster2009,
  author = {David Forster},
  title = {Robotic {Unicycle}},
  note = {Report, Department of Engineering, University of Cambridge, UK},
  year = {2009},
  owner = {marc},
  timestamp = {2009.08.10}
}

@INPROCEEDINGS{Quinonero-Candela2003a,
  author = {Joaquin {Qui{\~n}onero-Candela} and Agathe Girard and Jan Larsen
	and Carl E. Rasmussen},
  title = {Propagation {of Uncertainty in Bayesian Kernel Models---Application
	to Multiple-Step Ahead Forecasting}},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},
  year = {2003},
  volume = {2},
  pages = {701--704},
  month = {April},
  abstract = {The object of Bayesian modelling is the predictive distribution, which
	in a forecasting scenario enables improved estimates of forecasted
	values and their uncertainties. In this paper we focus on reliably
	estimating the predictive mean and variance of forecasted values
	using Bayesian kernel based models such as the Gaussian Process and
	the Relevance Vector Machine. We derive novel analytic expressions
	for the predictive mean and variance for Gaussian kernel shapes under
	the assumption of a Gaussian input distribution in the static case,
	and of a recursive Gaussian predictive density in iterative forecasting.
	The capability of the method is demonstrated for forecasting of time-series
	and compared to approximate methods.},
  doi = {10.1109/ICASSP.2003.1202463},
  file = {pdf2686.pdf:http\://www.kyb.mpg.de/publications/pdfs/pdf2686.pdf:PDF},
  owner = {marc},
  timestamp = {2008.02.06}
}

@BOOK{Rasmussen2006,
  title = {Gaussian {Processes for Machine Learning}},
  publisher = {The MIT Press},
  year = {2006},
  editor = {T. G. Dietterich},
  author = {Carl E. Rasmussen and Christopher K. I. Williams},
  series = {Adaptive Computation and Machine Learning},
  address = {Cambridge, MA, USA},
  owner = {deisenroth},
  timestamp = {2006.12.06},
  url = {http://www.gaussianprocess.org/gpml/chapters/}
}

@INCOLLECTION{Snelson2006,
  author = {Edward Snelson and Zoubin Ghahramani},
  title = {Sparse {Gaussian Processes using Pseudo-inputs}},
  booktitle = {Advances in {Neural Information Processing Systems} 18},
  publisher = {The MIT Press},
  year = {2006},
  editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
  pages = {1257--1264},
  address = {Cambridge, MA, USA},
  owner = {marc},
  timestamp = {2007.10.10},
  url = {http://books.nips.cc/papers/files/nips18/NIPS2005_0543.pdf}
}

@INPROCEEDINGS{Spong1995,
  author = {Mark W. Spong and Daniel J. Block},
  title = {The {Pendubot: A Mechatronic System for Control Research and Education}},
  booktitle = {Proceedings of the Conference on Decision and Control},
  year = {1995},
  pages = {555--557},
  doi = {10.1.1.56.7067},
  owner = {marc},
  timestamp = {2009.03.25},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.7067}
}

@BOOK{Sutton1998,
  title = {Reinforcement {Learning: An Introduction}},
  publisher = {The MIT Press},
  year = {1998},
  editor = {T. G. Dietterich},
  author = {Richard S. Sutton and Andrew G. Barto},
  owner = {deisenroth},
  timestamp = {2006.12.19},
  url = {http://www.cs.ualberta.ca/%7Esutton/book/ebook/the-book.html}
}

